{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "headline_classification_task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "10j8nOczO3YEaCPyEaF4nkwxEspN9A_y0",
      "authorship_tag": "ABX9TyPzavgGdI3TF9NIMrMnQiIS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanit456/NeuroSummary/blob/two/headline_classification_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLksFXdRTNHn",
        "colab_type": "text"
      },
      "source": [
        "# Load preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0the6StRsK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tqdm\n",
        "\n",
        "THAIGOV_PATH = '/content/drive/Shared drives/NeuroSummary/data/b_data_playground/thai_gov_split/'\n",
        "\n",
        "with open(THAIGOV_PATH + 'train.pkl', 'rb') as f:\n",
        "  train_df = pickle.load(f)\n",
        "\n",
        "with open(THAIGOV_PATH + 'val.pkl', 'rb') as f:\n",
        "  val_df = pickle.load(f)\n",
        "\n",
        "with open(THAIGOV_PATH + 'test.pkl', 'rb') as f:\n",
        "  test_df = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD2_y1PgTQac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(THAIGOV_PATH + 'train_stop.pkl', 'rb') as f:\n",
        "  train_stop_df = pickle.load(f)\n",
        "\n",
        "with open(THAIGOV_PATH + 'val_stop.pkl', 'rb') as f:\n",
        "  val_stop_df = pickle.load(f)\n",
        "\n",
        "with open(THAIGOV_PATH + 'test_stop.pkl', 'rb') as f:\n",
        "  test_stop_df = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6WioxO1WEUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_label(df_headline, df_content):\n",
        "  labels = []\n",
        "  for idx in tqdm.tqdm(range(len(df_content))):\n",
        "    tmp = []\n",
        "    for word in df_content[idx]:\n",
        "      if word in df_headline.iloc[idx]:\n",
        "        tmp.append(1)\n",
        "      else:\n",
        "        tmp.append(0)\n",
        "    labels.append(tmp)\n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUI7IrqYXmYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "16211675-3c00-4aa7-c579-6362ca3ffd40"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>headline</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8608</th>\n",
              "      <td>[กรมเจ้าท่า, จท., กระทรวงคมนาคม, อาศัย, อำนาจ,...</td>\n",
              "      <td>[กรมเจ้าท่า, ปรับปรุง, อัตรา, ค่า, โดยสาร, เรื...</td>\n",
              "      <td>ด้านเศรษฐกิจ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8593</th>\n",
              "      <td>[รอง, นรม, พล.อ.ฉัตรชัยฯ, เป็น, ประธาน, การ, ป...</td>\n",
              "      <td>[รอง, นรม, พล.อ.ฉัตรชัยฯ, เป็น, ประธาน, การ, ป...</td>\n",
              "      <td>ข่าวทำเนียบรัฐบาล</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10488</th>\n",
              "      <td>[พลเอก สุรศักดิ์ กาญจนรัตน์, รัฐมนตรี, ว่าการ,...</td>\n",
              "      <td>[รมว., ทส., เปิด, งาน, ประชารัฐร่วมใจ, คน, ลำ,...</td>\n",
              "      <td>ด้านความมั่นคง</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12624</th>\n",
              "      <td>[เมื่อ, วัน, ที่, 5, กุมภาพันธ์, 2561, เวลา, 1...</td>\n",
              "      <td>[นายก, รัฐมนตรี, ลง, พื้นที่, เยี่ยมชม, วิถี, ...</td>\n",
              "      <td>ด้านเศรษฐกิจ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17242</th>\n",
              "      <td>[กระทรวงดิจิทัลฯ, กระทรวงดิจิทัล, ฯ, สนับสนุน,...</td>\n",
              "      <td>[กระทรวงดิจิทัลฯ, หนุน, สดช., จับ, มือ, ทีโอที...</td>\n",
              "      <td>ด้านสังคม</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 content  ...              class\n",
              "8608   [กรมเจ้าท่า, จท., กระทรวงคมนาคม, อาศัย, อำนาจ,...  ...       ด้านเศรษฐกิจ\n",
              "8593   [รอง, นรม, พล.อ.ฉัตรชัยฯ, เป็น, ประธาน, การ, ป...  ...  ข่าวทำเนียบรัฐบาล\n",
              "10488  [พลเอก สุรศักดิ์ กาญจนรัตน์, รัฐมนตรี, ว่าการ,...  ...     ด้านความมั่นคง\n",
              "12624  [เมื่อ, วัน, ที่, 5, กุมภาพันธ์, 2561, เวลา, 1...  ...       ด้านเศรษฐกิจ\n",
              "17242  [กระทรวงดิจิทัลฯ, กระทรวงดิจิทัล, ฯ, สนับสนุน,...  ...          ด้านสังคม\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo95ioPvThGH",
        "colab_type": "text"
      },
      "source": [
        "# Create dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7f4SKVKTjCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = train_df \n",
        "val_set = val_df\n",
        "test_set = test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mA-QE7VTj3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "c0f261db-58fe-4956-d7b9-88fd9c763451"
      },
      "source": [
        "## ! use only n first words for headline generation\n",
        "def use_firt_n_words(df_content, n):\n",
        "  new_ls = []\n",
        "  for content in df_content:\n",
        "    new_ls.append(content[:n])\n",
        "  return new_ls\n",
        "\n",
        "words_50_train_contents = use_firt_n_words(train_set['content'], n=50)\n",
        "words_50_val_contents = use_firt_n_words(val_set['content'], n=50)\n",
        "words_50_test_contents = use_firt_n_words(test_set['content'], n=50)\n",
        "\n",
        "train_labels = create_label(train_set['headline'],  words_50_train_contents)\n",
        "val_labels = create_label(val_set['headline'],  words_50_val_contents)\n",
        "test_labels = create_label(test_set['headline'],  words_50_test_contents)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9219/9219 [00:03<00:00, 2469.14it/s]\n",
            "100%|██████████| 2271/2271 [00:00<00:00, 2500.78it/s]\n",
            "100%|██████████| 2271/2271 [00:00<00:00, 2531.47it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHalwf1DTj08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "adc39ffe-e77b-4528-d0bb-b495b4dc8673"
      },
      "source": [
        "import collections\n",
        "def create_index(input_data,threshold):\n",
        "    input_text = [data for data in input_data]\n",
        "    # counts of word type has to be above or equal threshold\n",
        "    words = [word for sublist in input_text for word in sublist]\n",
        "    print(\"words :\",words)\n",
        "    word_count_all = list()\n",
        "    word_count = list()\n",
        "    #use set and len to get the number of unique words\n",
        "    word_count_all.extend(collections.Counter(words).most_common(len(set(words))))\n",
        "    unkcnt = 0\n",
        "    for (word,cnt) in word_count_all:\n",
        "      if cnt >= threshold:\n",
        "        word_count.append((word,cnt))\n",
        "      else:\n",
        "        unkcnt+=cnt\n",
        "    #include a token for unknown word\n",
        "    word_count.append((\"UNK\",unkcnt))\n",
        "    #print out 10 most frequent words\n",
        "    print(\"top 10: \",word_count[:10])\n",
        "    print(\"bottom 10: \",word_count[-10:])\n",
        "    dictionary = dict()\n",
        "    dictionary[\"for_keras_zero_padding\"] = 0\n",
        "    for word in word_count:\n",
        "      dictionary[word[0]] = len(dictionary)\n",
        "    dictionary['<s>'] = len(dictionary) \n",
        "    dictionary['</s>'] = len(dictionary)\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    \n",
        "    # for data in input_data:\n",
        "    #   sub_data = list()\n",
        "    #   sub_label = list()\n",
        "    #   error_list = [data[i] for i in range(1,len(data))]\n",
        "\n",
        "    #   for i in range(len(data[0])):\n",
        "    #     sub_label.append(0)\n",
        "    #     for error_range in error_list:\n",
        "    #       if i>=error_range[0] and i<error_range[1]:\n",
        "    #         sub_label[-1] = 1\n",
        "    #         break.\n",
        "    return dictionary, reverse_dictionary\n",
        "dict_t, rev_dict_t = create_index(input_data=words_50_train_contents,threshold=0)\n",
        "# dict_stop_t, rev_dict_stop_t = create_index(input_data=words_70_train_stop_contents+words_20_train_stop_headlines,threshold=0)\n",
        "print('Vocab size (Content): ',len(dict_t))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OHB3hzUTjyS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "38cdcc10-e36b-46e3-d16b-7fec24ce57be"
      },
      "source": [
        "print('Vocab size: ',len(dict_t))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size:  19627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O1LEbZEslL7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b6f908da-5a58-4a9b-e427-3f77d1c32085"
      },
      "source": [
        "dict_t['<s>']\n",
        "list(dict_t.values())[-5:]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[19622, 19623, 19624, 19625, 19626]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BqI9HLgsnvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_to_idx(input_data,dictionary) :\n",
        "  X = list()\n",
        "  for data in input_data:\n",
        "    sub_data = []\n",
        "    for word in data:\n",
        "      if word in dictionary:\n",
        "        sub_data.append(dictionary[word])\n",
        "      else:\n",
        "        sub_data.append(dictionary[\"UNK\"])\n",
        "    X.append(sub_data)\n",
        "  return np.array(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6yGsptusqcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = word_to_idx(words_50_train_contents,dict_t)\n",
        "y_train = train_labels\n",
        "X_val = word_to_idx(words_50_val_contents,dict_t)\n",
        "y_val = val_labels\n",
        "X_test = word_to_idx(words_50_test_contents,dict_t)\n",
        "y_test = test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiFU41dd3f0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpbCYuo-svN6",
        "colab_type": "text"
      },
      "source": [
        "# Data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK7N6oTtsx1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, batch_size=32, dimX=70,dimY=20, n_channels=1,\n",
        "                 n_classes=10, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dimX = dimX\n",
        "        self.dimY = dimY\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "        label_temp = [self.labels[k] for k in indexes]\n",
        "        # Generate data\n",
        "        X, y_shift,y = self.__data_generation(list_IDs_temp,label_temp)\n",
        "        return [X,y_shift],y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, X,y):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        # X = np.empty((self.batch_size, self.dimX))\n",
        "        # y = np.empty((self.batch_size, self.dimY))\n",
        "\n",
        "        # Generate data\n",
        "        # for i, ID in enumerate(list_IDs_temp):\n",
        "        #     # Store sample\n",
        "        #     X[i,] = list_IDs_temp\n",
        "        \n",
        "        y = list(map(lambda mem :np.append(mem,[len(dict_t)-1]),y))\n",
        "        y_shift = list(map(lambda mem :np.append([len(dict_t)-2], mem[:-1]),y))\n",
        "        X = pad_sequences(X,maxlen=maxlen,padding='post')\n",
        "        y = pad_sequences(y,maxlen=maxlen_output,padding='post') \n",
        "        y_shift = pad_sequences(y_shift,maxlen=maxlen_output,padding='post')\n",
        "        # X = np.array(keras.utils.to_categorical(X, num_classes=self.n_classes), dtype=object)\n",
        "        y = np.array(keras.utils.to_categorical(y, num_classes=self.n_classes))\n",
        "        y_shift = np.array(keras.utils.to_categorical(y_shift, num_classes=self.n_classes))\n",
        "        return X, y_shift,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFkhxZlesyyC",
        "colab_type": "text"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPp6Bjvqs0kF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 50 \n",
        "maxlen_output = 20 # <s>ABC and ABC</s>\n",
        "vocab_size = len(dict_t)\n",
        "# output_vocab_size = len(dict_t)\n",
        "m=15000\n",
        "Tx=maxlen\n",
        "Ty=maxlen_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX_DF1HWs20P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply,Add,Conv1D,GRU,TimeDistributed\n",
        "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding,concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping, TensorBoard\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "import numpy as np\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRHoK-nss43U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # from my_classes import DataGenerator\n",
        "\n",
        "# # Parameters\n",
        "# params = {'dimX': maxlen,\n",
        "#           'dimY': maxlen_output,\n",
        "#           'batch_size': 512,\n",
        "#           'n_classes': vocab_size,\n",
        "#           'n_channels': 1,\n",
        "#           'shuffle': True}\n",
        "\n",
        "# # Generators\n",
        "# training_generator = DataGenerator(X_train, y_train, **params)\n",
        "# validation_generator = DataGenerator(X_val, y_val, **params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iocMyANRs8-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# callbacks\n",
        "curr_datetime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "drive_path = '/content/drive/My Drive'\n",
        "checkpoint_path=  drive_path + '/headline_thaigov_generation/weights/' + curr_datetime + \"_weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "\n",
        "\n",
        "callback_list = [     \n",
        "      ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min'), \n",
        "      EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50),\n",
        "      # ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=100, min_lr=1e-10),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAr5agIZx-pC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extractive_classifcation_model():\n",
        "  inputs = Input(shape=(maxlen, ))\n",
        "  x = Embedding(vocab_size, maxlen, trainable=True)(inputs)\n",
        "  x = Bidirectional(LSTM(maxlen, return_sequences=True))(x)\n",
        "  x = LSTM(maxlen, return_sequences=True)(x)\n",
        "  outputs = Dense(1, activation='sigmoid')(x)\n",
        "  model = Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.summary()\n",
        "  return Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNgqZ2Cy1DER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "2949ca55-de04-427e-d1d4-a42576bd5d7f"
      },
      "source": [
        "extractive_model = extractive_classifcation_model()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 50, 50)            981350    \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 50, 100)           40400     \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 50, 50)            30200     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 50, 1)             51        \n",
            "=================================================================\n",
            "Total params: 1,052,001\n",
            "Trainable params: 1,052,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhNZxMnG1DBJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "4fb8ee98-a85f-4f56-f58e-2f48b5e4a46f"
      },
      "source": [
        "extractive_model.fit(X_train, y_train, batch_size=32,  epochs=2000)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-96ed93db52a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextractive_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute '_in_multi_worker_mode'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX4YtglV1C8r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "6255a011-a3fc-41e2-981d-418a57bec48a"
      },
      "source": [
        ""
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M11h7Ghm1C4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5pF0Kwas9t4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def define_models(n_input=maxlen,n_output=maxlen_output,n_units=32) :\n",
        "#   #define training encoder model\n",
        "#   encoder_inputs = Input(shape=(maxlen,))\n",
        "#   encoder_embedding = Embedding(vocab_size, n_units)(encoder_inputs)\n",
        "#   encoder  = LSTM(n_units, return_state=True)\n",
        "#   encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
        "#   encoder_states = [state_h, state_c]\n",
        "\n",
        "#   # define training decoder model\n",
        "#   decoder_inputs = Input(shape=(maxlen_output,vocab_size ))\n",
        "#   # decoder_embedding = Embedding(vocab_size, n_units)\n",
        "#   # decoder_inputs2 = decoder_embedding(decoder_inputs)\n",
        "#   decoder_lstm = LSTM(n_units, return_state=True, return_sequences=True)\n",
        "#   decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "#   # dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
        "#   decoder_dense  = Dense(vocab_size, activation='softmax')\n",
        "#   decoder_outputs = decoder_dense(decoder_outputs)\n",
        "#   model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "#   #define inferencing encoder model\n",
        "#   encoder_model = Model(encoder_inputs,encoder_states)\n",
        "#   #define inference decoder\n",
        "#   decoder_state_input_h = Input(shape=(n_units,))\n",
        "#   decoder_state_input_c = Input(shape=(n_units,))\n",
        "#   decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "#   decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "#   decoder_states = [state_h, state_c]\n",
        "#   decoder_outputs = decoder_dense(decoder_outputs)\n",
        "#   decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "#   # return all models\n",
        "#   return model,encoder_model,decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFEPyNL-tEA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# configure problem\n",
        "# n_features = 50 + 1\n",
        "n_steps_in = maxlen\n",
        "n_steps_out = maxlen_output\n",
        "train, infenc, infdec = define_models(n_units=64)\n",
        "# train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "train.compile(loss='categorical_crossentropy', optimizer=\"rmsprop\", metrics=['accuracy'])\n",
        "train.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRpssltmtFyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.fit_generator(generator=training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    epochs=300, callbacks=callback_list)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}