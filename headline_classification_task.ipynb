{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "headline_classification_task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "10j8nOczO3YEaCPyEaF4nkwxEspN9A_y0",
      "authorship_tag": "ABX9TyMu471iRKj88uKh473CrvHq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanit456/NeuroSummary/blob/two/headline_classification_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLksFXdRTNHn",
        "colab_type": "text"
      },
      "source": [
        "# Load preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FynxtzIPcrgl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "21f556d4-2dd2-4a34-82f1-3f30d8180db7"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0-rc4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0the6StRsK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tqdm\n",
        "\n",
        "THAIGOV_PATH = '/content/drive/Shared drives/NeuroSummary/data/b_data_playground/thai_gov_split/'\n",
        "\n",
        "with open(THAIGOV_PATH + 'train.pkl', 'rb') as f:\n",
        "  train_df = pickle.load(f)\n",
        "\n",
        "with open(THAIGOV_PATH + 'val.pkl', 'rb') as f:\n",
        "  val_df = pickle.load(f)\n",
        "\n",
        "with open(THAIGOV_PATH + 'test.pkl', 'rb') as f:\n",
        "  test_df = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD2_y1PgTQac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(THAIGOV_PATH + 'train_stop.pkl', 'rb') as f:\n",
        "  train_stop_df = pickle.load(f)\n",
        "\n",
        "with open(THAIGOV_PATH + 'val_stop.pkl', 'rb') as f:\n",
        "  val_stop_df = pickle.load(f)\n",
        "\n",
        "with open(THAIGOV_PATH + 'test_stop.pkl', 'rb') as f:\n",
        "  test_stop_df = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6WioxO1WEUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_label(df_headline, df_content):\n",
        "  labels = []\n",
        "  for idx in tqdm.tqdm(range(len(df_content))):\n",
        "    tmp = []\n",
        "    for word in df_content[idx]:\n",
        "      if word in df_headline.iloc[idx]:\n",
        "        tmp.append(1)\n",
        "      else:\n",
        "        tmp.append(0)\n",
        "    labels.append(tmp)\n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUI7IrqYXmYD",
        "colab_type": "code",
        "outputId": "59ddcb62-8dc4-4e24-b2c3-cbba9e5cb2b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>headline</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8608</th>\n",
              "      <td>[กรมเจ้าท่า, จท., กระทรวงคมนาคม, อาศัย, อำนาจ,...</td>\n",
              "      <td>[กรมเจ้าท่า, ปรับปรุง, อัตรา, ค่า, โดยสาร, เรื...</td>\n",
              "      <td>ด้านเศรษฐกิจ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8593</th>\n",
              "      <td>[รอง, นรม, พล.อ.ฉัตรชัยฯ, เป็น, ประธาน, การ, ป...</td>\n",
              "      <td>[รอง, นรม, พล.อ.ฉัตรชัยฯ, เป็น, ประธาน, การ, ป...</td>\n",
              "      <td>ข่าวทำเนียบรัฐบาล</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10488</th>\n",
              "      <td>[พลเอก สุรศักดิ์ กาญจนรัตน์, รัฐมนตรี, ว่าการ,...</td>\n",
              "      <td>[รมว., ทส., เปิด, งาน, ประชารัฐร่วมใจ, คน, ลำ,...</td>\n",
              "      <td>ด้านความมั่นคง</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12624</th>\n",
              "      <td>[เมื่อ, วัน, ที่, 5, กุมภาพันธ์, 2561, เวลา, 1...</td>\n",
              "      <td>[นายก, รัฐมนตรี, ลง, พื้นที่, เยี่ยมชม, วิถี, ...</td>\n",
              "      <td>ด้านเศรษฐกิจ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17242</th>\n",
              "      <td>[กระทรวงดิจิทัลฯ, กระทรวงดิจิทัล, ฯ, สนับสนุน,...</td>\n",
              "      <td>[กระทรวงดิจิทัลฯ, หนุน, สดช., จับ, มือ, ทีโอที...</td>\n",
              "      <td>ด้านสังคม</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 content  ...              class\n",
              "8608   [กรมเจ้าท่า, จท., กระทรวงคมนาคม, อาศัย, อำนาจ,...  ...       ด้านเศรษฐกิจ\n",
              "8593   [รอง, นรม, พล.อ.ฉัตรชัยฯ, เป็น, ประธาน, การ, ป...  ...  ข่าวทำเนียบรัฐบาล\n",
              "10488  [พลเอก สุรศักดิ์ กาญจนรัตน์, รัฐมนตรี, ว่าการ,...  ...     ด้านความมั่นคง\n",
              "12624  [เมื่อ, วัน, ที่, 5, กุมภาพันธ์, 2561, เวลา, 1...  ...       ด้านเศรษฐกิจ\n",
              "17242  [กระทรวงดิจิทัลฯ, กระทรวงดิจิทัล, ฯ, สนับสนุน,...  ...          ด้านสังคม\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo95ioPvThGH",
        "colab_type": "text"
      },
      "source": [
        "# Create dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7f4SKVKTjCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = train_df \n",
        "val_set = val_df\n",
        "test_set = test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mA-QE7VTj3h",
        "colab_type": "code",
        "outputId": "d22b7582-f66d-4f7f-f278-29df9e9953d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "## ! use only n first words for headline generation\n",
        "def use_firt_n_words(df_content, n):\n",
        "  new_ls = []\n",
        "  for content in df_content:\n",
        "    new_ls.append(content[:n])\n",
        "  return new_ls\n",
        "\n",
        "words_300_train_contents = use_firt_n_words(train_set['content'], n=300)\n",
        "words_300_val_contents = use_firt_n_words(val_set['content'], n=300)\n",
        "words_300_test_contents = use_firt_n_words(test_set['content'], n=300)\n",
        "\n",
        "train_labels = create_label(train_set['headline'],  words_300_train_contents)\n",
        "val_labels = create_label(val_set['headline'],  words_300_val_contents)\n",
        "test_labels = create_label(test_set['headline'],  words_300_test_contents)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9219/9219 [00:26<00:00, 344.57it/s]\n",
            "100%|██████████| 2271/2271 [00:06<00:00, 353.65it/s]\n",
            "100%|██████████| 2271/2271 [00:06<00:00, 351.50it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHalwf1DTj08",
        "colab_type": "code",
        "outputId": "d400e8d7-25cb-4f7b-b149-574b84bdbb61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "import collections\n",
        "def create_index(input_data,threshold):\n",
        "    input_text = [data for data in input_data]\n",
        "    # counts of word type has to be above or equal threshold\n",
        "    words = [word for sublist in input_text for word in sublist]\n",
        "    print(\"words :\",words)\n",
        "    word_count_all = list()\n",
        "    word_count = list()\n",
        "    #use set and len to get the number of unique words\n",
        "    word_count_all.extend(collections.Counter(words).most_common(len(set(words))))\n",
        "    unkcnt = 0\n",
        "    for (word,cnt) in word_count_all:\n",
        "      if cnt >= threshold:\n",
        "        word_count.append((word,cnt))\n",
        "      else:\n",
        "        unkcnt+=cnt\n",
        "    #include a token for unknown word\n",
        "    word_count.append((\"UNK\",unkcnt))\n",
        "    #print out 10 most frequent words\n",
        "    print(\"top 10: \",word_count[:10])\n",
        "    print(\"bottom 10: \",word_count[-10:])\n",
        "    dictionary = dict()\n",
        "    dictionary[\"for_keras_zero_padding\"] = 0\n",
        "    for word in word_count:\n",
        "      dictionary[word[0]] = len(dictionary)\n",
        "    dictionary['<s>'] = len(dictionary) \n",
        "    dictionary['</s>'] = len(dictionary)\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    \n",
        "    # for data in input_data:\n",
        "    #   sub_data = list()\n",
        "    #   sub_label = list()\n",
        "    #   error_list = [data[i] for i in range(1,len(data))]\n",
        "\n",
        "    #   for i in range(len(data[0])):\n",
        "    #     sub_label.append(0)\n",
        "    #     for error_range in error_list:\n",
        "    #       if i>=error_range[0] and i<error_range[1]:\n",
        "    #         sub_label[-1] = 1\n",
        "    #         break.\n",
        "    return dictionary, reverse_dictionary\n",
        "dict_t, rev_dict_t = create_index(input_data=words_300_train_contents,threshold=0)\n",
        "# dict_stop_t, rev_dict_stop_t = create_index(input_data=words_70_train_stop_contents+words_20_train_stop_headlines,threshold=0)\n",
        "print('Vocab size (Content): ',len(dict_t))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "top 10:  [('การ', 94387), ('และ', 61697), ('ที่', 52967), ('ใน', 49600), ('ให้', 34379), ('มี', 32016), ('ความ', 31579), ('เป็น', 29185), ('ได้', 28895), ('ของ', 26326)]\n",
            "bottom 10:  [('มโนสุจริต', 1), ('อิจฉา', 1), ('ริษยา', 1), ('พยาบาท', 1), ('ภูมิแผ่นดิน', 1), ('เอ็กซ์', 1), ('โป@', 1), ('นายพีระ อัครวัตร', 1), ('ส่นแยก', 1), ('UNK', 0)]\n",
            "Vocab size (Content):  48044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OHB3hzUTjyS",
        "colab_type": "code",
        "outputId": "c399bcff-9a23-41c0-ae0d-502263dabd76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print('Vocab size: ',len(dict_t))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size:  48044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O1LEbZEslL7",
        "colab_type": "code",
        "outputId": "c4dc3d4d-f146-43ae-d788-fd1e167fa83b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "dict_t['<s>']\n",
        "list(dict_t.values())[-5:]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[48039, 48040, 48041, 48042, 48043]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BqI9HLgsnvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_to_idx(input_data,dictionary) :\n",
        "  X = list()\n",
        "  for data in input_data:\n",
        "    sub_data = []\n",
        "    for word in data:\n",
        "      if word in dictionary:\n",
        "        sub_data.append(dictionary[word])\n",
        "      else:\n",
        "        sub_data.append(dictionary[\"UNK\"])\n",
        "    X.append(sub_data)\n",
        "  return np.array(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6yGsptusqcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = word_to_idx(words_300_train_contents,dict_t)\n",
        "y_train = train_labels\n",
        "X_val = word_to_idx(words_300_val_contents,dict_t)\n",
        "y_val = val_labels\n",
        "X_test = word_to_idx(words_300_test_contents,dict_t)\n",
        "y_test = test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpbCYuo-svN6",
        "colab_type": "text"
      },
      "source": [
        "# Data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK7N6oTtsx1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
        "\n",
        "# class DataGenerator(keras.utils.Sequence):\n",
        "#     'Generates data for Keras'\n",
        "#     def __init__(self, list_IDs, labels, batch_size=32, dimX=70,dimY=20, n_channels=1,\n",
        "#                  n_classes=10, shuffle=True):\n",
        "#         'Initialization'\n",
        "#         self.dimX = dimX\n",
        "#         self.dimY = dimY\n",
        "#         self.batch_size = batch_size\n",
        "#         self.labels = labels\n",
        "#         self.list_IDs = list_IDs\n",
        "#         self.n_channels = n_channels\n",
        "#         self.n_classes = n_classes\n",
        "#         self.shuffle = shuffle\n",
        "#         self.on_epoch_end()\n",
        "\n",
        "#     def __len__(self):\n",
        "#         'Denotes the number of batches per epoch'\n",
        "#         return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         'Generate one batch of data'\n",
        "#         # Generate indexes of the batch\n",
        "#         indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "#         # Find list of IDs\n",
        "#         list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "#         label_temp = [self.labels[k] for k in indexes]\n",
        "#         # Generate data\n",
        "#         X, y_shift,y = self.__data_generation(list_IDs_temp,label_temp)\n",
        "#         return [X,y_shift],y\n",
        "\n",
        "#     def on_epoch_end(self):\n",
        "#         'Updates indexes after each epoch'\n",
        "#         self.indexes = np.arange(len(self.list_IDs))\n",
        "#         if self.shuffle == True:\n",
        "#             np.random.shuffle(self.indexes)\n",
        "\n",
        "#     def __data_generation(self, X,y):\n",
        "#         'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "#         # Initialization\n",
        "#         # X = np.empty((self.batch_size, self.dimX))\n",
        "#         # y = np.empty((self.batch_size, self.dimY))\n",
        "\n",
        "#         # Generate data\n",
        "#         # for i, ID in enumerate(list_IDs_temp):\n",
        "#         #     # Store sample\n",
        "#         #     X[i,] = list_IDs_temp\n",
        "        \n",
        "#         # y = list(map(lambda mem :np.append(mem,[len(dict_t)-1]),y))\n",
        "#         # y_shift = list(map(lambda mem :np.append([len(dict_t)-2], mem[:-1]),y))\n",
        "#         X = pad_sequences(X,maxlen=maxlen,padding='post')\n",
        "#         y = pad_sequences(y,maxlen=maxlen,padding='post')\n",
        "#         # y = pad_sequences(y,maxlen=maxlen_output,padding='post') \n",
        "#         # y_shift = pad_sequences(y_shift,maxlen=maxlen_output,padding='post')\n",
        "#         # X = np.array(keras.utils.to_categorical(X, num_classes=self.n_classes), dtype=object)\n",
        "#         # y = np.array(keras.utils.to_categorical(y, num_classes=self.n_classes))\n",
        "#         # y_shift = np.array(keras.utils.to_categorical(y_shift, num_classes=self.n_classes))\n",
        "#         return X,y "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFkhxZlesyyC",
        "colab_type": "text"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPp6Bjvqs0kF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 300\n",
        "maxlen_output = 300\n",
        "vocab_size = len(dict_t)\n",
        "# output_vocab_size = len(dict_t)\n",
        "m=15000\n",
        "Tx=maxlen\n",
        "Ty=maxlen_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX_DF1HWs20P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply,Add,Conv1D,GRU,TimeDistributed\n",
        "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding,concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping, TensorBoard\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "import numpy as np\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRHoK-nss43U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # from my_classes import DataGenerator\n",
        "\n",
        "# # Parameters\n",
        "# params = {'dimX': maxlen,\n",
        "#           'dimY': maxlen_output,\n",
        "#           'batch_size': 512,\n",
        "#           'n_classes': vocab_size,\n",
        "#           'n_channels': 1,\n",
        "#           'shuffle': True}\n",
        "\n",
        "# # Generators\n",
        "# training_generator = DataGenerator(X_train, y_train, **params)\n",
        "# validation_generator = DataGenerator(X_val, y_val, **params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iocMyANRs8-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# callbacks\n",
        "curr_datetime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "drive_path = '/content/drive/My Drive'\n",
        "checkpoint_path=  drive_path + '/headline_thaigov_classification/weights/' + curr_datetime + \"_weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "\n",
        "\n",
        "callback_list = [     \n",
        "      ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min'), \n",
        "      EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100),\n",
        "      # ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=100, min_lr=1e-10),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAr5agIZx-pC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "def extractive_classifcation_model():\n",
        "  inputs = Input(shape=(maxlen, ))\n",
        "  x = Embedding(vocab_size, maxlen, trainable=True)(inputs)\n",
        "  x = LSTM(maxlen, return_sequences=True)(x)\n",
        "  x = LSTM(maxlen, return_sequences=True)(x) \n",
        "  outputs = TimeDistributed(Dense(1, activation='sigmoid'))(x)\n",
        "  model = Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(optimizer=RMSprop(learning_rate=1e-8), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNgqZ2Cy1DER",
        "colab_type": "code",
        "outputId": "b7ee664e-5f9e-41be-e906-cd32bd6eb85b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "extractive_model = extractive_classifcation_model()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 300)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 300, 300)          14413200  \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 300, 300)          721200    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 300, 300)          721200    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 300, 1)            301       \n",
            "=================================================================\n",
            "Total params: 15,855,901\n",
            "Trainable params: 15,855,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tie0_GFZW77h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded_X_train = np.array(pad_sequences(X_train, maxlen=maxlen, padding='post'))\n",
        "padded_X_val = np.array(pad_sequences(X_val, maxlen=maxlen, padding='post'))\n",
        "padded_X_test = np.array(pad_sequences(X_test, maxlen=maxlen, padding='post'))\n",
        "\n",
        "padded_y_train = np.array(pad_sequences(y_train, maxlen=maxlen_output, padding='post'))\n",
        "padded_y_val = np.array(pad_sequences(y_val, maxlen=maxlen_output, padding='post'))\n",
        "padded_y_test = np.array(pad_sequences(y_test, maxlen=maxlen_output, padding='post'))\n",
        "\n",
        "reshaped_y_train = padded_y_train.reshape(-1, maxlen_output, 1)\n",
        "reshaped_y_val = padded_y_val.reshape(-1, maxlen_output, 1)\n",
        "reshaped_y_test = padded_y_test.reshape(-1, maxlen_output, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kreh_p3OZdtG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "d1c59cd9-9965-42ca-aecc-63bf1381d718"
      },
      "source": [
        "padded_X_train[:5]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2922,  6309,   778, ...,  2635,   131,     3],\n",
              "       [   81, 23356,  7270, ...,    46,    69,    19],\n",
              "       [ 1212,    16,    67, ...,     8,   125,   129],\n",
              "       [   97,    18,     3, ...,     0,     0,     0],\n",
              "       [  836,   501,   107, ...,   426,  5678,    45]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrG58m8FaJHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "032f282b-1d46-47d2-f208-d9ec75836067"
      },
      "source": [
        "padded_X_val[:5]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4723, 11146,   361, ...,     0,     0,     0],\n",
              "       [  342,   123,   556, ...,    14,     5,     7],\n",
              "       [   18,    19,   583, ...,     0,     0,     0],\n",
              "       [ 1555,     4,   737, ...,     0,     0,     0],\n",
              "       [  309,    58,    16, ...,   600,    49,    55]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1tBpwnNa08_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e4657a2a-1a5c-4915-a4e5-27f1c6b7640f"
      },
      "source": [
        "extractive_model"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.training.Model at 0x7f0c2b2b89e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhNZxMnG1DBJ",
        "colab_type": "code",
        "outputId": "400f4cef-35b2-4819-c96d-d5d24b1f4dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "K.clear_session()\n",
        "extractive_model.fit(padded_X_train, reshaped_y_train, batch_size=1024, validation_data=(padded_X_val, reshaped_y_val), epochs=1000,\n",
        "                     callbacks=callback_list)\n",
        "# extractive_model.fit(, y_train, batch_size = 32,epochs = 1)\n",
        "# extractive_model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8501e-08 - accuracy: 0.5849\n",
            "Epoch 00001: val_loss improved from inf to 0.00000, saving model to /content/drive/My Drive/headline_thaigov_classification/weights/20200509-174356_weights-improvement-01-0.00.hdf5\n",
            "10/10 [==============================] - 8s 825ms/step - loss: 1.8501e-08 - accuracy: 0.5849 - val_loss: 1.8548e-08 - val_accuracy: 0.5726\n",
            "Epoch 2/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8501e-08 - accuracy: 0.5849\n",
            "Epoch 00002: val_loss did not improve from 0.00000\n",
            "10/10 [==============================] - 7s 651ms/step - loss: 1.8501e-08 - accuracy: 0.5849 - val_loss: 1.8548e-08 - val_accuracy: 0.5726\n",
            "Epoch 3/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8501e-08 - accuracy: 0.5849\n",
            "Epoch 00003: val_loss did not improve from 0.00000\n",
            "10/10 [==============================] - 7s 654ms/step - loss: 1.8501e-08 - accuracy: 0.5849 - val_loss: 1.8548e-08 - val_accuracy: 0.5726\n",
            "Epoch 4/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8501e-08 - accuracy: 0.5849\n",
            "Epoch 00004: val_loss did not improve from 0.00000\n",
            "10/10 [==============================] - 7s 652ms/step - loss: 1.8501e-08 - accuracy: 0.5849 - val_loss: 1.8548e-08 - val_accuracy: 0.5726\n",
            "Epoch 5/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8501e-08 - accuracy: 0.5849\n",
            "Epoch 00005: val_loss did not improve from 0.00000\n",
            "10/10 [==============================] - 7s 651ms/step - loss: 1.8501e-08 - accuracy: 0.5849 - val_loss: 1.8548e-08 - val_accuracy: 0.5726\n",
            "Epoch 6/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8501e-08 - accuracy: 0.5849\n",
            "Epoch 00006: val_loss did not improve from 0.00000\n",
            "10/10 [==============================] - 7s 650ms/step - loss: 1.8501e-08 - accuracy: 0.5849 - val_loss: 1.8548e-08 - val_accuracy: 0.5726\n",
            "Epoch 7/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8501e-08 - accuracy: 0.5849\n",
            "Epoch 00007: val_loss did not improve from 0.00000\n",
            "10/10 [==============================] - 7s 651ms/step - loss: 1.8501e-08 - accuracy: 0.5849 - val_loss: 1.8548e-08 - val_accuracy: 0.5726\n",
            "Epoch 8/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8501e-08 - accuracy: 0.5849\n",
            "Epoch 00008: val_loss did not improve from 0.00000\n",
            "10/10 [==============================] - 7s 654ms/step - loss: 1.8501e-08 - accuracy: 0.5849 - val_loss: 1.8548e-08 - val_accuracy: 0.5726\n",
            "Epoch 9/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8501e-08 - accuracy: 0.5849\n",
            "Epoch 00009: val_loss did not improve from 0.00000\n",
            "10/10 [==============================] - 7s 650ms/step - loss: 1.8501e-08 - accuracy: 0.5849 - val_loss: 1.8548e-08 - val_accuracy: 0.5726\n",
            "Epoch 10/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8501e-08 - accuracy: 0.5849\n",
            "Epoch 00010: val_loss did not improve from 0.00000\n",
            "10/10 [==============================] - 6s 649ms/step - loss: 1.8501e-08 - accuracy: 0.5849 - val_loss: 1.8548e-08 - val_accuracy: 0.5726\n",
            "Epoch 11/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8501e-08 - accuracy: 0.5849\n",
            "Epoch 00011: val_loss did not improve from 0.00000\n",
            "10/10 [==============================] - 6s 650ms/step - loss: 1.8501e-08 - accuracy: 0.5849 - val_loss: 1.8548e-08 - val_accuracy: 0.5726\n",
            "Epoch 12/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8501e-08 - accuracy: 0.5849\n",
            "Epoch 00012: val_loss did not improve from 0.00000\n",
            "10/10 [==============================] - 7s 653ms/step - loss: 1.8501e-08 - accuracy: 0.5849 - val_loss: 1.8548e-08 - val_accuracy: 0.5726\n",
            "Epoch 13/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.8501e-08 - accuracy: 0.5849\n",
            "Epoch 00013: val_loss did not improve from 0.00000\n",
            "10/10 [==============================] - 7s 653ms/step - loss: 1.8501e-08 - accuracy: 0.5849 - val_loss: 1.8548e-08 - val_accuracy: 0.5726\n",
            "Epoch 14/1000\n",
            " 4/10 [===========>..................] - ETA: 3s - loss: 1.8586e-08 - accuracy: 0.5828"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-28449659457a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m extractive_model.fit(padded_X_train, reshaped_y_train, batch_size=1024, validation_data=(padded_X_val, reshaped_y_val), epochs=1000,\n\u001b[0;32m----> 3\u001b[0;31m                      callbacks=callback_list)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# extractive_model.fit(, y_train, batch_size = 32,epochs = 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# extractive_model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3x331kxdu_N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "d5677151-d4df-4cd5-922a-e644ca1e6c5e"
      },
      "source": [
        "extractive_model.evaluate(padded_X_test, reshaped_y_test)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71/71 [==============================] - 2s 24ms/step - loss: 1.8948e-08 - accuracy: 0.5713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.894751378017645e-08, 0.5712798833847046]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC8yDI5Jjvs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = extractive_model.predict(padded_X_test)\n",
        "y_pred = np.where(y_pred >= 0.5, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOJsY0tSkFxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_df.head()\n",
        "first_300_words = []\n",
        "for i in test_df['content']:\n",
        "  first_300_words.append([i[:300]])\n",
        "test_df['first_300_words_content'] = first_300_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_ZM35bjknB-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "67c837cb-ec31-4c52-f593-61ff25a7ee45"
      },
      "source": [
        "def extractive_text(y_pred, chunk_text):\n",
        "  ls = []\n",
        "  for i in tqdm.tqdm(range(len(chunk_text))):\n",
        "    tmp = []\n",
        "    for j in range(len(chunk_text[i][0])):\n",
        "      if y_pred[i][j] == 1:\n",
        "        tmp.append(chunk_text[i][0][j])\n",
        "    ls.append(tmp)\n",
        "  return ls\n",
        "\n",
        "np_test_df = test_df['first_300_words_content'].to_numpy()\n",
        "extraction = extractive_text(y_pred, np_test_df)\n",
        "\n",
        "for i in range(5):\n",
        "  print('Content (at most 300 words) :', test_df.iloc[i]['first_300_words_content'])\n",
        "  print('Headline                    :', test_df.iloc[i]['headline'])\n",
        "  print('Prediction.                 :', extraction[i])\n",
        "  print('Labels                      :', *y_pred[i])\n",
        "  print()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2271/2271 [00:00<00:00, 2537.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Content (at most 300 words) : [['นางอภิรดี ตันตราภรณ์', 'รัฐมนตรี', 'ว่าการ', 'กระทรวงพาณิชย์', 'ใน', 'ฐานะ', 'ประธาน', 'คณะ', 'กรรมการ', 'พิจารณา', 'การ', 'ทุ่ม', 'ตลาด', 'และ', 'การ', 'อุดหนุน', 'ทตอ.', 'ได้', 'เปิดเผย', 'ว่า', 'เมื่อ', 'ต้น', 'เดือน', 'มีนาคม', '2560', 'ที่', 'ผ่าน', 'มา', 'ได้', 'มี', 'การ', 'ประชุม', 'คณะ', 'กรรมการ ทตอ.', 'ซึ่ง', 'มี', 'มติ', 'ดัง', 'นี้', '1', '.', 'ให้', 'ขยาย', 'ระยะ', 'เวลา', 'การ', 'ใช้', 'มาตรการ', 'ชั่วคราว', 'ตอบโต้', 'การ', 'ทุ่มตลาด', 'ออก', 'ไป', 'อีก', '2', 'เดือน', 'ของ', 'สินค้า', '2', 'รายการ', 'ดัง', 'ต่อ', 'ไป', 'นี้', '1', '.', '1', 'สินค้า', 'หลอด', 'และ', 'ท่อ', 'ทำ', 'ด้วย', 'เหล็ก', 'หรือ', 'เหล็กกล้า', 'ที่', 'มี', 'แหล่ง', 'กำเนิด', 'จาก', 'สาธารณรัฐประชาชนจีน', 'และ', 'สาธารณรัฐเกาหลี', '1', '.', '2', 'สินค้า', 'เหล็ก', 'แผ่น', 'รีดร้อน', 'ชนิด', 'เป็นม้วน', 'และ', 'ไม่', 'เป็นม้วน', 'ที่', 'มี', 'แหล่ง', 'กำเนิด', 'จาก', 'สหพันธ์สาธารณรัฐบราซิล', 'สาธารณรัฐอิสลาม', 'อิหร่าน', 'และ', 'สาธารณรัฐตุรกี', '2', '.', 'ให้', 'เรียก', 'เก็บ', 'อากร', 'ตอบโต้', 'การ', 'ทุ่ม', 'ตลาด', 'สินค้า', 'เหล็ก', 'แผ่น', 'รีด', 'เย็น', 'ชุบ', 'หรือ', 'เคลือบ', 'ด้วย', 'โลหะ', 'เจือ', 'ของ', 'อะลูมิเนียม', 'และ', 'สังกะสี', 'แบบ', 'จุ่มร้อน', 'GL', 'ที่', 'มี', 'แหล่ง', 'กำเนิด', 'จาก', 'สาธารณรัฐ', 'สังคม', 'นิยม', 'เวียดนาม', 'เป็น', 'ระยะ', 'เวลา', '5', 'ปี', 'ใน', 'อัตรา', 'ร้อย', 'ละ', '6', '.', '20', '–', '40', '.', '49', 'ของ', 'ราคา', 'ซี', 'ไอ', 'เอฟ', ' 3', '.', 'ให้', 'เรียก', 'เก็บ', 'อากร', 'ตอบโต้', 'การ', 'ทุ่ม', 'ตลาด', 'สินค้า', 'เหล็ก', 'แผ่น', 'รีดเย็น', 'เคลือบ', 'ด้วย', 'สังกะสี', 'แบบ', 'จุ่มร้อน', 'แล้ว', 'ทาสี', 'และ', 'เหล็ก', 'แผ่น', 'รีด', 'เย็น', 'ชุบ', 'หรือ', 'เคลือบ', 'ด้วย', 'โลหะ', 'เจือ', 'ของ', 'อะลูมิเนียม', 'และ', 'สังกะสี', 'แบบ', 'จุ่มร้อน', 'แล้ว', 'ทาสี', 'PPGI', '/', 'PPGL', 'ที่', 'มี', 'แหล่ง', 'กำเนิด', 'จาก', 'สาธารณรัฐ', 'สังคม', 'นิยม', 'เวียดนาม', 'เป็น', 'ระยะ', 'เวลา', '5', 'ปี', 'ใน', 'อัตรา', 'ร้อย', 'ละ', '4', '.', '30', '–', '60', '.', '26', 'ของ', 'ราคา', 'ซี ไอ เอฟ', 'นอก', 'จาก', 'นี้', 'ยัง', 'เห็น', 'ชอบ', 'ร่าง', 'ผล', 'การ', 'ทบทวน', 'การ', 'เรียก', 'เก็บ', 'อากร', 'ตอบโต้', 'การ', 'ทุ่มตลาด', 'ใน', 'อัตรา', 'ร้อย', 'ละ', '0', 'ของ', 'ราคา', 'ซี ไอ', 'เอฟ', 'สำหรับ', 'สินค้า', 'เหล็ก', 'ลวดคาร์บอนต่ำ', 'ฯ', 'จาก', 'จีน', 'ภาย', 'ใต้', 'พิกัด', '7227', '.', '9000', '.', '090', 'ซึ่ง', 'จะ', 'ได้', 'มี', 'การ', 'ดำเนิน', 'การ', 'จัด', 'รับฟัง', 'ความ', 'คิดเห็น', 'Public', 'Hearing', 'ต่อ', 'ไป', 'รัฐมนตรี', 'ว่าการ', 'กระทรวงพาณิชย์', 'กล่าว', 'ด้วย', 'ว่า', 'การ', 'พิจารณา']]\n",
            "Headline                    : ['มติ', 'การ', 'ประชุม', 'คณะ', 'กรรมการ', 'พิจารณา', 'การ', 'ทุ่ม', 'ตลาด', 'และ', 'การ', 'อุดหนุน']\n",
            "Prediction.                 : ['นางอภิรดี ตันตราภรณ์', 'กระทรวงพาณิชย์', 'ใน', 'ฐานะ', 'ประธาน', 'คณะ', 'กรรมการ', 'พิจารณา', 'การ', 'มา', 'ได้', 'มี', '2', 'เดือน', 'ของ', 'สินค้า', '2', 'รายการ', 'ดัง', 'ต่อ', 'นี้', '1', 'หรือ', 'เหล็กกล้า', 'ที่', 'มี', 'แหล่ง', 'กำเนิด', 'จาก', 'สาธารณรัฐประชาชนจีน', 'และ', 'สาธารณรัฐเกาหลี', '1', '.', '2', 'รีดร้อน', 'ชนิด', 'เป็นม้วน', 'และ', 'ไม่', 'เป็นม้วน', 'ที่', 'มี', 'แหล่ง', 'กำเนิด', 'จาก', 'สหพันธ์สาธารณรัฐบราซิล', 'สาธารณรัฐอิสลาม', 'อิหร่าน', 'และ', 'สาธารณรัฐตุรกี', 'หรือ', 'เคลือบ', 'ด้วย', 'โลหะ', 'เจือ', 'ของ', 'อะลูมิเนียม', 'และ', 'สังกะสี', 'แบบ', 'จุ่มร้อน', 'GL', 'ที่', 'มี', 'แหล่ง', 'กำเนิด', 'จาก', 'สาธารณรัฐ', 'สังคม', 'นิยม', 'เวียดนาม', 'เป็น', 'ระยะ', 'เวลา', '5', 'ปี', 'ใน', 'อัตรา', 'ร้อย', '6', '.', '20', '–', '40', '.', 'ของ', 'ราคา', 'ซี', 'ไอ', 'เอฟ', ' 3', '.', 'ให้', 'เรียก', 'แล้ว', 'ทาสี', 'และ', 'เหล็ก', 'แผ่น', 'รีด', 'เย็น', 'ชุบ', 'หรือ', 'เคลือบ', 'ด้วย', 'โลหะ', 'เจือ', 'ของ', 'อะลูมิเนียม', 'และ', 'สังกะสี', 'แบบ', 'จุ่มร้อน', 'แล้ว', 'ทาสี', 'PPGI', '/', 'PPGL', 'ที่', 'มี', 'แหล่ง', 'กำเนิด', 'จาก', 'สาธารณรัฐ', 'สังคม', 'นิยม', 'เวียดนาม', 'เป็น', 'ระยะ', 'เวลา', '5', 'ปี', 'ใน', 'อัตรา', 'ร้อย', '4', '.', '30', '–', '60', 'ของ', 'ราคา', 'ซี ไอ เอฟ', 'นอก', 'จาก', 'นี้', 'ยัง', 'เห็น', 'ชอบ', 'ร่าง', 'ผล', 'การ', 'ทบทวน', 'ราคา', 'ซี ไอ', 'เอฟ', 'สำหรับ', 'สินค้า', 'เหล็ก', 'ลวดคาร์บอนต่ำ', 'ฯ', 'จาก', 'จีน', 'ภาย', 'ใต้', 'พิกัด', '7227', '.', '9000', '.', '090', 'ซึ่ง', 'จะ', 'ได้', 'กล่าว', 'ด้วย', 'ว่า', 'การ', 'พิจารณา']\n",
            "Labels                      : [1] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [0] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [1] [1] [1] [1] [1] [1] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [1] [1] [1] [1] [1] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1]\n",
            "\n",
            "Content (at most 300 words) : [['เมื่อ', 'วัน', 'ที่', '2', 'มีนาคม', '2560', '/', 'กระทรวงวิทยาศาสตร์และ', 'เทคโนโลยี', 'โดย', 'ศูนย์', 'ความ', 'เป็น', 'เลิศ', 'ด้าน', 'ชีววิทยาศาสตร์', 'องค์การมหาชน', 'TCELS', 'ร่วม', 'กับ', 'บริษัท บัวหลวงเวนเจอร์ส จำกัด', 'ร่วม', 'ผนึก', 'กำลัง', 'ขับเคลื่อน', 'เส้นทาง', 'ธุรกิจ', 'Life', 'Sciences', 'Startups', 'สู่', 'มิติ', 'ใหม่', 'แห่ง', 'วงการ', 'ธุรกิจ', 'จัด', 'กิจกรรม', '“', 'TCELS', 'Life', 'Science', '&', 'MedTech ', 'Acceleration', 'Program', '”', 'ภาย', 'ใต้', 'โครงการ', 'Thailand', 'Startup', 'for', 'Life', 'Sciences', '2017', 'โดย', 'มี', 'ดร.อรรชกา สีบุญเรือง', 'รัฐมนตรี', 'ว่าการ', 'กระทรวงวิทยาศาสตร์และ', 'เทคโนโลยี', 'เป็น', 'ประธาน', 'ณ', 'ห้อง', 'วี', 'ฟังก์ชัน', '1', 'และ', '2', 'ชั้น', '12', 'โรง', 'แรม', 'วี', 'กรุงเทพฯ', 'เอ็มแกลลอรี่', ' บาย', 'โซฟิเทล', 'สำหรับ', 'งาน', 'นี้', 'ได้', 'เชิญชวน', 'ผู้', 'ประกอบ', 'การ', 'รุ่น', 'ใหม่', 'บริษัท Startup', 'นัก', 'วิจัย', 'นัก', 'ศึกษา', 'ที่', 'ทา', 'งาน', 'ด้าน', 'เครื่องมือ', 'แพทย์', 'Medical', 'Devices', 'เครื่อง', 'สำอาง', 'Cosmetics', 'หุ่น', 'ยนต์', 'ทาง', 'การ', 'แพทย์', 'Medical', 'Robotics', 'และ', 'ทุก', 'ด้าน', 'ที่', 'เกี่ยวข้อง', 'กับ', 'สุขภาพ', 'Health', 'and', 'Wellness', 'เข้า', 'ร่วม', 'สมัคร', 'โครงการ', 'Thailand', 'Startup', 'for', 'Life', 'Sciences', '2017', 'เพื่อ', 'คัดเลือก', 'เข้า', 'อบรม', 'ใน', 'หลักสูตร', 'เร่ง', 'การ', 'เติบโต', 'หรือ', 'ที่', 'เรียก', 'ว่า', 'Life', 'Sciences', 'and', 'Med', 'Tech', 'Acceleration', 'Program', 'โดย', 'การ', 'อบรม', 'จะ', 'ควบคู่', 'ไป', 'กับ', 'การ', 'ให้', 'คำ', 'ปรึกษาเพื่อ', 'พัฒนา', 'ผลิตภัณฑ์', 'ต้น', 'แบบ', 'สู่', 'แผน', 'ธุรกิจ', 'ที่', 'ดึงดูด', 'นัก', 'ลง', 'ทุน', 'พร้อม', 'กัน', 'นี้', 'ยัง', 'มี', 'การ', 'สนับสนุน', 'ทุน', 'เร่ง', 'การ', 'เติบโต', 'และ', 'ผลักดัน', 'ให้', 'เกิด', 'การ', 'ลง', 'ทุน', 'ของ', 'เวนเจอร์', 'ส', 'ระ', 'ดับ', 'แนว', 'หน้า', 'อีก', 'ด้วย', 'ดร.อรรชกา สีบุญเรือง', 'รัฐมนตรี', 'ว่าการ', 'กระทรวงวิทยาศาสตร์และ', 'เทคโนโลยี', 'กล่าว', 'ว่า', 'จาก', 'นโยบาย', 'ของ', 'คณะ', 'รัฐมนตรี', 'โดย', 'ฯพณฯ', 'ท่าน', 'นายก', 'รัฐมนตรี', 'พลเอกประยุทธ จันทร์โอชา', 'ที่', 'เล็ง', 'เห็น', 'ถึง', 'ความ', 'สำคัญ', 'ใน', 'การ', 'พัฒนา', 'ประเทศ', 'ทั้ง', 'ทาง', 'ด้าน', 'เศรษฐกิจ', 'และ', 'สังคม', 'ให้', 'ประเทศ', 'มั่นคง', 'ประชาชน', 'มั่งคั่ง', 'อย่าง', 'ยั่งยืน', 'ด้วย', 'การ', 'เร่ง', 'สร้าง', 'ขีด', 'ความ', 'สามารถ', 'ด้าน', 'วิทยาศาสตร์ เทคโนโลยี', 'วิจัย', 'และ', 'นวัตกรรม', 'ของ', 'ประเทศไทย', 'ซึ่ง', 'ได้', 'กำหนด', 'เป้าหมาย', 'ไว้', '3', 'ด้าน', 'คือ', 'เพิ่ม', 'สัดส่วน', 'ค่า', 'ใช้จ่าย', 'การ', 'ลง', 'ทุน', 'เพื่อ', 'การ', 'วิจัย', 'และ', 'พัฒนา', 'สู่', 'ร้อย', 'ละ', '1', '.', '5', 'ของ', 'GDP', 'และ', 'มี', 'สัดส่วน', 'การ', 'ลง', 'ทุน', 'วิจัย', 'และ', 'พัฒนา', 'ของ', 'ภาค', 'เอกชน', 'ต่อ']]\n",
            "Headline                    : ['ก.วิทย์', 'ฯ', 'โดย', 'TCELS', 'จัด', 'แถลง', 'TCELS', 'Life', 'Science', '&', 'MedTech ', 'Acceleration', 'Program']\n",
            "Prediction.                 : ['เมื่อ', 'ใหม่', 'แห่ง', 'วงการ', 'ธุรกิจ', 'จัด', 'กิจกรรม', '“', 'ใต้', 'โครงการ', 'Thailand', 'Startup', 'for', '2017', 'โดย', 'มี', 'ดร.อรรชกา สีบุญเรือง', 'รัฐมนตรี', 'ว่าการ', 'กระทรวงวิทยาศาสตร์และ', 'เทคโนโลยี', 'เป็น', 'ประธาน', 'ณ', 'ห้อง', 'วี', 'ฟังก์ชัน', '1', 'และ', '2', 'ชั้น', '12', 'โรง', 'แรม', ' บาย', 'โซฟิเทล', 'สำหรับ', 'งาน', 'นี้', 'ได้', 'เชิญชวน', 'ผู้', 'ประกอบ', 'การ', 'ใหม่', 'บริษัท Startup', 'งาน', 'Devices', 'เครื่อง', 'สำอาง', 'เพื่อ', 'คัดเลือก', 'เข้า', 'อบรม', 'ใน', 'หลักสูตร', 'เร่ง', 'การ', 'หรือ', 'ที่', 'เรียก', 'ว่า', 'Life', 'Sciences', 'and', 'Med', 'Tech', 'Acceleration', 'Program', 'โดย', 'การ', 'อบรม', 'ที่', 'ดึงดูด', 'นัก', 'ดร.อรรชกา สีบุญเรือง', 'รัฐมนตรี', 'ว่าการ', 'กระทรวงวิทยาศาสตร์และ', 'เทคโนโลยี', 'กล่าว', 'ว่า', 'จาก', 'นโยบาย', 'ของ', 'คณะ', 'รัฐมนตรี', 'โดย', 'ฯพณฯ', 'ท่าน', 'นายก', 'รัฐมนตรี', 'พลเอกประยุทธ จันทร์โอชา', 'ที่', 'เล็ง', 'เห็น', 'ถึง', 'กำหนด', 'เป้าหมาย', 'ไว้', '3', 'ด้าน', 'คือ', 'เพิ่ม', 'สัดส่วน', 'ค่า', 'ใช้จ่าย', 'การ', 'ลง', 'ทุน', 'เพื่อ', 'สัดส่วน', 'ต่อ']\n",
            "Labels                      : [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [1] [1] [0] [0] [0] [0] [0] [0] [1] [0] [0] [0] [0] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1]\n",
            "\n",
            "Content (at most 300 words) : [['กระทรวงทรัพยากรธรรมชาติ', 'และสิ่ง', 'แวดล้อม', 'ลง', 'พื้นที่', 'จ.ลำพูน', 'เพื่อ', 'รับฟัง', 'ข้อมูล', 'จาก', 'ภาค', 'ส่วน', 'ต่าง', 'ๆ', 'ใน', 'พื้นที่', 'เมื่อ', 'เร็ว', 'ๆ', 'นี้', 'พลเอก สุรศักดิ์ กาญจนรัตน์', 'รัฐมนตรี', 'ว่าการ', 'กระทรวงทรัพยากรธรรมชาติ', 'และสิ่ง', 'แวดล้อม', 'พร้อม', 'ด้วย', 'นายธัญญา เนติธรรมกุล', 'อธิบดี', 'กรมอุทยานแห่งชาติ สัตว์ป่า และ', 'พันธุ์พืช', 'และ', 'ผู้', 'บริหาร', 'ระดับ', 'สูง', 'ลง', 'พื้นที่', 'บ้านแม่สะแงะ', 'อำเภอแม่ทา', 'จังหวัดลำพูน', 'ใน', 'เขต', 'รักษาพันธุ์สัตว์ป่าดอยผาเมือง', 'เพื่อ', 'รับฟัง', 'ข้อมูล', 'จาก', 'ภาค', 'ส่วน', 'ต่างๆ', 'ใน', 'พื้นที่', 'พร้อม', 'ฟัง', 'บท', 'สรุป', 'จาก', 'เวที', 'ถอด', 'บท', 'เรียน', 'จาก', 'เวที', 'ประชาชน', 'รวม', 'ทั้ง', 'พบปะ', 'เด็ก', 'นัก', 'เรียน', 'และ', 'ประชาชน']]\n",
            "Headline                    : ['รมว.', 'ทส.', 'ลง', 'พื้นที่', 'จังหวัดลำพูน', 'และ', 'เชียงใหม่', 'เพื่อ', 'รับฟัง', 'ข้อมูล', 'จาก', 'ภาค', 'ส่วน', 'ต่าง', 'ๆ', 'ใน', 'พื้นที่']\n",
            "Prediction.                 : ['และสิ่ง', 'จ.ลำพูน', 'เพื่อ', 'รับฟัง', 'ข้อมูล', 'จาก', 'ภาค', 'ส่วน', 'ต่าง', 'ๆ', 'ใน', 'พื้นที่', 'เมื่อ', 'เร็ว', 'ๆ', 'นี้', 'พลเอก สุรศักดิ์ กาญจนรัตน์', 'รัฐมนตรี', 'ว่าการ', 'กระทรวงทรัพยากรธรรมชาติ', 'และสิ่ง', 'แวดล้อม', 'พร้อม', 'ด้วย', 'นายธัญญา เนติธรรมกุล', 'อธิบดี', 'กรมอุทยานแห่งชาติ สัตว์ป่า และ', 'พันธุ์พืช', 'และ', 'ผู้', 'บริหาร', 'ระดับ', 'สูง', 'เขต', 'รักษาพันธุ์สัตว์ป่าดอยผาเมือง', 'เพื่อ', 'รับฟัง', 'ข้อมูล', 'จาก', 'ภาค', 'ส่วน', 'ต่างๆ', 'ใน', 'พื้นที่', 'พร้อม', 'ฟัง', 'บท', 'สรุป', 'จาก', 'เวที', 'ถอด', 'บท', 'เรียน']\n",
            "Labels                      : [0] [1] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]\n",
            "\n",
            "Content (at most 300 words) : [['นายชาญเชาวน์ ไชยานุกิจ', 'ปลัด', 'กระทรวงยุติธรรม', 'เป็น', 'ประธาน', 'การ', 'ประชุม', 'ขับเคลื่อน', 'เชิง', 'นโยบาย', 'เรือน', 'จำสุขภาวะ', 'ครั้ง', 'ที่', '2', 'เรื่อง', '“', 'สาน', 'พลัง', 'ชุมชน', 'เพื่อ', 'คืน', 'คน', 'ดี', 'สู่', 'สังคม', 'แต่', 'กฎหมาย', 'กลับ', 'ห้าม', 'คน', 'พ้น', 'โทษ', 'ทำ', 'งาน', '”', 'เมื่อ', 'วัน', 'พุธ', 'ที่', '25', 'มกราคม', '2560', 'เวลา', '09', '.', '30', 'น.', 'นายชาญเชาวน์ ไชยานุกิจ', 'ปลัด', 'กระทรวงยุติธรรม', 'เป็น', 'ประธาน', 'การ', 'ประชุม', 'ขับเคลื่อน', 'เชิง', 'นโยบาย', 'เรือน', 'จำสุขภาวะ', 'ครั้ง', 'ที่', '2', 'เรื่อง', '“', 'สาน', 'พลัง', 'ชุมชน', 'เพื่อ', 'คืน', 'คน', 'ดี', 'สู่', 'สังคม', 'แต่', 'กฎหมาย', 'กลับ', 'ห้าม', 'คน', 'พ้น', 'โทษ', 'ทำ', 'งาน', '”', 'เพื่อ', 'พัฒนา', 'เครือข่ายพันธมิตร', 'และ', 'ภาค', 'ส่วน', 'ที่', 'เกี่ยวข้อง', 'ให้', 'เข้า', 'มา', 'มี', 'ส่วน', 'ร่วม', 'ใน', 'งาน', 'ราชทัณฑ์', 'เพื่อ', 'ลดอคติ', 'ทาง', 'สังคม', 'และ', 'ให้', 'ผู้', 'ต้อง', 'ขัง', '/', 'ผู้', 'พ้น', 'โทษ', 'ได้', 'รับ', 'การ', 'ยอม', 'รับ', 'ใน', 'ฐานะ', 'พลเมือง', 'ที่', 'มี', 'บทบาท', 'หน้าที่', 'รับผิดชอบ', 'ใน', 'สังคม', 'จัด', 'โดย', 'สมาคม', 'นักวิจัย', 'ประชากร', 'และ', 'สังคม', 'สถาบันวิจัยประชากรและสังคม', 'มหาวิทยาลัยมหิดล', 'สภา', 'วิชาชีพ', 'สังคมสงเคราะห์', 'และ', 'กรมราชทัณฑ์', 'กระทรวงยุติธรรม', 'สนับสนุน', 'โดย', 'สำนักงานกองทุนสนับสนุนการ', 'เสริมสร้างสุขภาพ', 'สสส.', 'โดย', 'มี', 'นายชาติชาย สุทธิกลม', 'กรรมการ', 'สิทธิมนุษยชนแห่งชาติ', 'และ', 'รอง', 'ศาสตราจารย์ ดร.กฤตยา อาชวนิจกุล', 'นายก', 'สมาคมนักวิจัย', 'ประชากรและ', 'สังคม', 'พร้อม', 'ด้วย', 'ผู้', 'แทน', 'จาก', 'ภาค', 'ส่วน', 'ต่างๆ', 'ที่', 'เกี่ยวข้อง', 'เข้า', 'ร่วม', 'ฯ', 'ณ', 'ห้อง', 'ประชุม', '709', 'สำนักงานคณะกรรมการสิทธิมนุษยชนแห่งชาติ', 'อาคาร', 'รัฐประศาสนภักดี', 'ศูนย์ราชการ', 'เฉลิม', 'พระเกียรติฯ']]\n",
            "Headline                    : ['ปลัด', 'กระทรวงยุติธรรม', 'ประชุม', 'ขับเคลื่อน', 'เชิง', 'นโยบาย', 'เรือน', 'จำสุขภาวะ', 'พัฒนา', 'เครือข่ายพันธมิตร', 'และ', 'ภาค', 'ส่วน', 'ที่', 'เกี่ยวข้อง', '“', 'คืน', 'คน', 'ดี', 'สู่', 'สังคม', '”']\n",
            "Prediction.                 : ['ปลัด', 'กระทรวงยุติธรรม', 'เป็น', 'ประธาน', 'การ', 'ประชุม', 'นโยบาย', 'เรือน', 'จำสุขภาวะ', 'ครั้ง', 'ที่', '2', 'เรื่อง', '“', 'สาน', 'พลัง', 'ชุมชน', 'เพื่อ', 'คืน', 'คน', 'ดี', 'สู่', 'สังคม', 'แต่', 'กฎหมาย', 'กลับ', 'ห้าม', 'คน', 'พ้น', 'โทษ', 'ทำ', 'งาน', '”', 'เมื่อ', 'วัน', 'พุธ', 'ที่', 'มกราคม', '2560', 'เวลา', '09', '.', '30', 'น.', 'นายชาญเชาวน์ ไชยานุกิจ', 'ปลัด', 'กระทรวงยุติธรรม', 'เป็น', 'ประธาน', 'การ', 'ประชุม', 'ขับเคลื่อน', 'เชิง', 'นโยบาย', 'เรือน', 'จำสุขภาวะ', 'ครั้ง', 'ที่', '2', 'เรื่อง', '“', 'สาน', 'พลัง', 'ชุมชน', 'เพื่อ', 'คืน', 'คน', 'ดี', 'สู่', 'สังคม', 'แต่', 'กฎหมาย', 'กลับ', 'ห้าม', 'คน', 'พ้น', 'โทษ', 'ทำ', 'งาน', '”', 'เพื่อ', 'พัฒนา', 'เครือข่ายพันธมิตร', 'และ', 'ภาค', 'ส่วน', 'ที่', 'เกี่ยวข้อง', 'ให้', 'เข้า', 'มา', 'มี', 'ส่วน', 'ร่วม', 'ใน', 'งาน', 'ราชทัณฑ์', 'เพื่อ', 'ลดอคติ', 'ทาง', 'สังคม', 'และ', 'ให้', 'ผู้', 'ต้อง', 'ขัง', '/', 'รับ', 'ใน', 'ฐานะ', 'พลเมือง', 'ที่', 'มี', 'บทบาท', 'หน้าที่', 'รับผิดชอบ', 'ใน', 'สังคม', 'จัด', 'โดย', 'สมาคม', 'นักวิจัย', 'ประชากร', 'และ', 'สังคม', 'สถาบันวิจัยประชากรและสังคม', 'มหาวิทยาลัยมหิดล', 'สภา', 'วิชาชีพ', 'สังคมสงเคราะห์', 'และ', 'กรมราชทัณฑ์', 'กระทรวงยุติธรรม', 'สนับสนุน', 'โดย', 'สำนักงานกองทุนสนับสนุนการ', 'เสริมสร้างสุขภาพ', 'สสส.', 'โดย', 'มี', 'นายชาติชาย สุทธิกลม', 'รอง', 'ศาสตราจารย์ ดร.กฤตยา อาชวนิจกุล', 'ต่างๆ', 'ที่', 'เกี่ยวข้อง', 'เข้า', 'ร่วม', 'ฯ', 'ณ', 'ห้อง', 'ประชุม', '709', 'สำนักงานคณะกรรมการสิทธิมนุษยชนแห่งชาติ', 'อาคาร', 'รัฐประศาสนภักดี', 'ศูนย์ราชการ', 'เฉลิม', 'พระเกียรติฯ']\n",
            "Labels                      : [0] [1] [1] [1] [1] [1] [1] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]\n",
            "\n",
            "Content (at most 300 words) : [['ตาม', 'ที่', 'ปรากฏ', 'ข่าว', 'ทาง', 'หน้า', 'นสพ.', 'ใน', 'ประเด็น', 'กรมธนารักษ์', 'เตรียม', 'การ', 'ผลิต', 'เหรียญ', 'กษาปณ์', 'หมุนเวียน', 'ชุด', 'ใหม่', 'ชนิด', 'ราคา', '10', 'บาท', '5', 'บาท', '2', 'บาท', '1', 'บาท', '50', 'สตางค์', '25', 'สตางค์', '10', 'สตางค์', '5', 'สตางค์', 'และ', '1', 'สตางค์', 'ออก', 'ใช้', 'หมุนเวียน', 'ใน', 'ระบบ', 'เศรษฐกิจ', 'ใน', 'ปี', 'งบ', 'ประมาณ', '2561', 'วัน', 'นี้', '16', 'ตุลาคม', '1560', 'ณ', 'กรมธนารักษ์', 'นายพชร อนันตศิลป์', 'อธิบดี', 'กรมธนารักษ์', 'กล่าว', 'ว่า', 'ตาม', 'ที่', 'ปรากฏ', 'ข่าว', 'ทาง', 'หน้า', 'หนังสือพิมพ์', 'ใน', 'ประเด็น', 'กรมธนารักษ์', 'เตรียม', 'การ', 'ผลิต', 'เหรียญ', 'กษาปณ์', 'หมุนเวียน', 'ชุด', 'ใหม่', 'ชนิด', 'ราคา', '10', 'บาท', '5', 'บาท', '2', 'บาท', '1', 'บาท', '50', 'สตางค์', '25', 'สตางค์', '10', 'สตางค์', '5', 'สตางค์', 'และ', '1', 'สตางค์', 'ออก', 'ใช้', 'หมุนเวียน', 'ใน', 'ระบบ', 'เศรษฐกิจ', 'ใน', 'ปี', 'งบ', 'ประมาณ', '2561', 'รวม', 'ทั้ง', 'การ', 'จัดทำ', 'เหรียญ', 'ที่', 'ระลึก', 'พระราชพิธี', 'บรมราชาภิเษก', 'ชนิด', 'ทองคำ', 'พ่น', 'ทราย', 'ชนิด', 'เงิน', 'รม', 'ดำ', 'พ่น', 'ทราย', 'และ', 'ชนิด', 'ทองแดง', 'รม', 'ดำ', 'พ่น', 'ทราย', 'นั้น', 'นายพชร', 'กล่าว', 'ต่อ', 'ว่า', 'กรมธนารักษ์', 'ได้', 'รับ', 'พระราชทาน', 'พระราชานุญาต', 'ให้', 'จัดทำ', 'เหรียญ', 'หมุนเวียน', 'ชุด', 'ใหม่', 'เพื่อ', 'ใช้', 'หมุนเวียน', 'ใน', 'ระบบ', 'เศรษฐกิจ', 'ของ', 'ประเทศ', 'ทุก', 'ชนิด', 'ราคา', 'แล้ว', 'ขณะ', 'นี้', 'อยู่', 'ใน', 'ระหว่าง', 'นำ', 'เสนอ', 'คณะ', 'รัฐมนตรี', 'เพื่อ', 'ออก', 'กฎ', 'กระทรวง', 'กำหนด', 'ลักษณะ', 'ของ', 'เหรียญ', 'กษาปณ์', 'เพื่อ', 'กำหนด', 'ชนิด', 'ราคา', 'โลหะ', 'อัตรา', 'เนื้อ', 'โลหะ', 'น้ำหนัก', 'ขนาด', 'ลวดลาย', 'รวม', 'ทั้ง', 'อัตรา', 'เผื่อ', 'เหลือ', 'เผื่อ', 'ขาด', 'ของ', 'เหรียญ', 'กษาปณ์', 'หมุนเวียน', 'แต่ละ', 'ชนิด', 'ราคา', 'เพื่อ', 'ให้', 'เป็น', 'ไป', 'ตาม', 'พระราชบัญญัติ', 'เงินตรา', 'พ.ศ.', '2501', 'ข้อ', '10', 'สำหรับ', 'การ', 'จัดทำ', 'เหรียญ', 'ที่', 'ระลึก', 'พระราชพิธี', 'บรมราชาภิเษก', 'นั้น', 'ความ', 'คืบหน้า', 'ขณะ', 'นี้', 'เป็น', 'เพียง', 'กรมธนารักษ์', 'ได้', 'รับ', 'พระราชทาน', 'พระราชวินิจฉัย', 'การ', 'ออก', 'แบบ', 'พระบรมฉายาลักษณ์', 'ที่', 'จะ', 'ประทับ', 'บน', 'เหรียญ', 'เท่า', 'นั้น', 'ยัง', 'มิ', 'ได้', 'มี', 'การ', 'กำหนด', 'ประเภท', 'ของ', 'เหรียญ', 'รวม', 'ทั้ง', 'ขนาด', 'น้ำหนัก', 'ของ', 'เหรียญ', 'ที่', 'จะ', 'จัดทำ', 'แต่อย่างใด', 'ซึ่ง', 'การ', 'กำหนด', 'รายละเอียด', 'ดัง', 'กล่าว', 'จะ', 'ต้อง', 'ผ่าน', 'การ', 'พิจารณา', 'กลั่นกรอง', 'โดย', 'คณะ', 'กรรมการ', 'วาง', 'แผน', 'เหรียญ', 'กษาปณ์', 'และ', 'คณะกรรรมการ', 'ที่ปรึกษา', 'เหรียญ', 'กษาปณ์', 'ที่', 'ระลึก', 'หรือ', 'เหรียญ', 'ที่', 'ระลึก']]\n",
            "Headline                    : ['ชี้แจง', 'ข่าว', 'กรณี', 'กรมธนารักษ์', 'ผลิต', 'เหรียญ', 'กษาปณ์', 'หมุนเวียน', 'ชุด', 'ใหม่', 'และ', 'เหรียญ', 'ที่', 'ระลึก', 'พระราชพิธี', 'บรมราชาภิเษก']\n",
            "Prediction.                 : ['ที่', 'ปรากฏ', 'ข่าว', 'ทาง', 'หน้า', 'นสพ.', 'ใน', 'ประเด็น', 'กรมธนารักษ์', 'เตรียม', 'การ', 'กษาปณ์', 'หมุนเวียน', 'ชุด', 'ใหม่', 'ชนิด', 'ราคา', '10', 'บาท', '5', 'บาท', '2', 'บาท', '1', 'บาท', '50', 'สตางค์', '25', 'สตางค์', 'กล่าว', 'ว่า', 'ตาม', 'ที่', 'ปรากฏ', 'ข่าว', 'ทาง', 'หน้า', 'หนังสือพิมพ์', 'ใน', 'ประเด็น', 'กรมธนารักษ์', 'เตรียม', 'การ', 'ผลิต', 'เหรียญ', 'กษาปณ์', 'หมุนเวียน', 'ชุด', 'ใหม่', 'ชนิด', 'ราคา', '10', 'บาท', '5', 'บาท', '2', 'บาท', '1', 'บาท', '50', 'สตางค์', '25', 'สตางค์', 'ที่', 'ระลึก', 'พระราชพิธี', 'บรมราชาภิเษก', 'ชนิด', 'ทองคำ', 'พ่น', 'ว่า', 'กรมธนารักษ์', 'ได้', 'รับ', 'พระราชทาน', 'พระราชานุญาต', 'ให้', 'จัดทำ', 'เหรียญ', 'หมุนเวียน', 'ชุด', 'ใหม่', 'เพื่อ', 'ใช้', 'หมุนเวียน', 'ใน', 'ระบบ', 'เศรษฐกิจ', 'ของ', 'ประเทศ', 'ทุก', 'ชนิด', 'ราคา', 'แล้ว', 'ขณะ', 'นี้', 'อยู่', 'ใน', 'ระหว่าง', 'นำ', 'เสนอ', 'คณะ', 'รัฐมนตรี', 'เพื่อ', 'ออก', 'กฎ', 'กระทรวง', 'กำหนด', 'ลักษณะ', 'ของ', 'เหรียญ', 'กษาปณ์', 'เพื่อ', 'กำหนด', 'ชนิด', 'ราคา', 'โลหะ', 'อัตรา', 'เนื้อ', 'โลหะ', 'น้ำหนัก', 'ขนาด', 'ลวดลาย', 'รวม', 'ทั้ง', 'อัตรา', 'เผื่อ', 'เหลือ', 'เผื่อ', 'ขาด', 'ของ', 'เหรียญ', 'กษาปณ์', 'หมุนเวียน', 'แต่ละ', 'ชนิด', 'ราคา', 'เพื่อ', 'ให้', 'เป็น', 'ไป', 'ตาม', 'พระราชบัญญัติ', 'เงินตรา', 'พ.ศ.', '2501', 'ข้อ', '10', 'สำหรับ', 'ที่', 'ระลึก', 'พระราชพิธี', 'บรมราชาภิเษก', 'นั้น', 'ความ', 'คืบหน้า', 'ขณะ', 'นี้', 'เป็น', 'เพียง', 'กรมธนารักษ์', 'พระราชทาน', 'ประทับ', 'บน', 'เหรียญ', 'เท่า', 'นั้น', 'ยัง', 'มิ', 'ได้', 'มี', 'การ', 'กำหนด', 'ประเภท', 'ของ', 'เหรียญ', 'รวม', 'ทั้ง', 'ขนาด', 'น้ำหนัก', 'ของ', 'เหรียญ', 'ที่', 'จะ', 'จัดทำ', 'แต่อย่างใด', 'ซึ่ง', 'การ', 'กำหนด', 'รายละเอียด', 'ดัง', 'กล่าว', 'จะ', 'ต้อง', 'ผ่าน', 'การ', 'กษาปณ์', 'และ', 'คณะกรรรมการ', 'ที่ปรึกษา', 'เหรียญ', 'กษาปณ์', 'ที่', 'ระลึก', 'หรือ', 'เหรียญ', 'ที่', 'ระลึก']\n",
            "Labels                      : [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [1] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [0] [0] [0] [0] [0] [0] [0] [0] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1] [1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxGNSQfSnaCl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0224a216-ee11-48ab-afaf-5feaff1c81ef"
      },
      "source": [
        "len(np_test_df[0][0])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX4YtglV1C8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls = [1,2,3,4]\n",
        "res = []\n",
        "res.append(ls)\n",
        "res.append(ls)\n",
        "res = np.array(res)\n",
        "res.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M11h7Ghm1C4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls = []\n",
        "for i in y_train:\n",
        "  print(np.array(i))\n",
        "  ls.append(np.array(i))\n",
        "  break\n",
        "ls = np.array(ls)\n",
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5pF0Kwas9t4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def define_models(n_input=maxlen,n_output=maxlen_output,n_units=32) :\n",
        "#   #define training encoder model\n",
        "#   encoder_inputs = Input(shape=(maxlen,))\n",
        "#   encoder_embedding = Embedding(vocab_size, n_units)(encoder_inputs)\n",
        "#   encoder  = LSTM(n_units, return_state=True)\n",
        "#   encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
        "#   encoder_states = [state_h, state_c]\n",
        "\n",
        "#   # define training decoder model\n",
        "#   decoder_inputs = Input(shape=(maxlen_output,vocab_size ))\n",
        "#   # decoder_embedding = Embedding(vocab_size, n_units)\n",
        "#   # decoder_inputs2 = decoder_embedding(decoder_inputs)\n",
        "#   decoder_lstm = LSTM(n_units, return_state=True, return_sequences=True)\n",
        "#   decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "#   # dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
        "#   decoder_dense  = Dense(vocab_size, activation='softmax')\n",
        "#   decoder_outputs = decoder_dense(decoder_outputs)\n",
        "#   model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "#   #define inferencing encoder model\n",
        "#   encoder_model = Model(encoder_inputs,encoder_states)\n",
        "#   #define inference decoder\n",
        "#   decoder_state_input_h = Input(shape=(n_units,))\n",
        "#   decoder_state_input_c = Input(shape=(n_units,))\n",
        "#   decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "#   decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "#   decoder_states = [state_h, state_c]\n",
        "#   decoder_outputs = decoder_dense(decoder_outputs)\n",
        "#   decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "#   # return all models\n",
        "#   return model,encoder_model,decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFEPyNL-tEA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# configure problem\n",
        "# n_features = 50 + 1\n",
        "n_steps_in = maxlen\n",
        "n_steps_out = maxlen_output\n",
        "train, infenc, infdec = define_models(n_units=64)\n",
        "# train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "train.compile(loss='categorical_crossentropy', optimizer=\"rmsprop\", metrics=['accuracy'])\n",
        "train.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRpssltmtFyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.fit_generator(generator=training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    epochs=300, callbacks=callback_list)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}